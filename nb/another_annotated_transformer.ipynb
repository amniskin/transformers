{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aec5c787-83aa-4d86-af51-ba8316842317",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Another annotated transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5296304c-30bd-4dfb-bdaa-eaaa9214dd17",
   "metadata": {
    "tags": []
   },
   "source": [
    "If you're anything like me, while trying to implement transformers, you've read the original [attention is all you need](https://arxiv.org/abs/1706.03762) paper, [the annotated transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html), [the updated version](http://nlp.seas.harvard.edu/annotated-transformer/), [d2l.ai](https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html), and had to cobble them all together to get something going. This post is an attempt to make that proccess easier for people like me in a short and to-the-point style. You can think of this as a bare-bones implementation with a whole lot of documentation.\n",
    "\n",
    "Notes:\n",
    "1. this post is about *how* transformers are implemented, not *why* they're implemented the way they are.\n",
    "2. This post pretty much takes you from just the basics of ML through understanding how transformers work. Feel free to skip whatever sections you're familiar with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe30876-afa4-4c66-aa8a-1603872d7a49",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Overview\n",
    "\n",
    "The following is the transformer architecture diagram taken from the original paper. We'll be referring back to it often in this post.\n",
    "\n",
    "![image.png](../_static/post/another-annotated-transformer/tformer.png)\n",
    "Transformer architecture diagram\n",
    "\n",
    "At a high level, the transformer is an encoder-decoder model; it takes a **sequence** of **tokens** from a source (e.g. English words) and learns to translate that into a destination sequence (e.g. French words). The encoder is the left side and the decoder is the right side of the architecture diagram.\n",
    "\n",
    "For the rest of the post, when we refer to \"tokens\" or \"sequences\", feel free to replace them with \"words\" and \"sentences\" (or \"paragraphs\", \"pages\", \"books\", etc.) respectively in your head."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e08a5-46dd-4c82-a63a-dff6bd706c9b",
   "metadata": {},
   "source": [
    "# Preamble\n",
    "\n",
    "We'll be using [jax](https://jax.readthedocs.io/en/latest/index.html) and [flax](https://flax.readthedocs.io/en/latest/index.html) for this implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d449674-4fbe-4f23-b4b3-9b117329d58b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import chex\n",
    "from flax import linen as nn, core\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jran\n",
    "import jax.tree_util\n",
    "from math import prod\n",
    "from typing import Callable, Sequence\n",
    "\n",
    "key = jran.PRNGKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885512ff-682d-40c6-baac-4aba5de87618",
   "metadata": {},
   "source": [
    "# Input (output) embeddings\n",
    "![image.png](../_static/post/another-annotated-transformer/embed-circled.png)\n",
    "\n",
    "Let's start with the beginning of the data flow: the Input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6522c40a-134c-48c9-b620-2365251b7901",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "The vocabulary we'll be using is digits 0-9. We then add a **pad** token (`\"p\"`) and a **start** token (`\"s\"`), both of which we'll explain later. So our tokens are `[\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"p\", \"s\"]`. Our first task is to turn this into something numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613f8c56-af9f-4274-a392-eb2e1a8e40c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 0,\n",
       " '1': 1,\n",
       " '2': 2,\n",
       " '3': 3,\n",
       " '4': 4,\n",
       " '5': 5,\n",
       " '6': 6,\n",
       " '7': 7,\n",
       " '8': 8,\n",
       " '9': 9,\n",
       " 'p': 10,\n",
       " 's': 11}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {str(x): i for i, x in enumerate(list(range(10)) + ['p', 's'])}\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86670f31-8986-443b-852e-1ee398e3d403",
   "metadata": {},
   "source": [
    "Now we can turn a number into a sequence of token indices like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b21e08bb-a4e3-4857-8048-98e366e5c173",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([2, 3, 2], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num2tokens(num):\n",
    "    return jnp.array([vocab.get(x) for x in str(num)])\n",
    "\n",
    "num2tokens(232)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d7756-c506-44db-b467-c900556bfadb",
   "metadata": {},
   "source": [
    "## Input embedding\n",
    "\n",
    "An embedding is a learned mapping from tokens to (hopefully) more densly populated vectors. Under the hood it's stored as a matrix but that's an implementation detail. Below is a model that embeds a vocabulary with 5 unique tokens to 2 dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc57adc0-6fb9-46ca-98d7-3146cc34a41d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = nn.Embed(len(vocab), 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6715b-f74e-4a09-933c-8f443abb735e",
   "metadata": {},
   "source": [
    "Note that we can easily inspect the parameters of a model, and that the **embedding** parameter below is a $5\\times 2$ matrix. It's $5\\times2$ because it's storing 5 independent 2-dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "247abcf8-df9f-4136-971f-d74319cabce3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        embedding: Array([[-0.07138442,  0.31376272],\n",
       "               [ 0.4053369 ,  0.9015168 ],\n",
       "               [-0.5195726 ,  0.2808921 ],\n",
       "               [-0.9512113 ,  0.35392332],\n",
       "               [-1.8304102 , -0.332351  ],\n",
       "               [ 0.40043908,  0.08431281],\n",
       "               [-0.4319639 , -0.7836565 ],\n",
       "               [ 0.54686207,  0.35685003],\n",
       "               [ 0.07191442, -0.1766023 ],\n",
       "               [-0.48776352,  0.33678803],\n",
       "               [ 1.6915971 ,  0.6755828 ],\n",
       "               [ 0.07079946,  0.05526797]], dtype=float32),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = model.init(key, jnp.array([1]))\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66484eb6-fb2f-4deb-a6e3-927ed8477752",
   "metadata": {},
   "source": [
    "To get embeddings from tokens, we can either do it ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33990b09-c934-40dd-8bc6-c51a5044700d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.5195726 ,  0.2808921 ],\n",
       "       [-0.5195726 ,  0.2808921 ],\n",
       "       [-0.9512113 ,  0.35392332],\n",
       "       [-1.8304102 , -0.332351  ]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.one_hot(num2tokens(2234), len(vocab)) @ params['params']['embedding']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bbf7c5-1327-4688-9853-238f2fa3ed89",
   "metadata": {},
   "source": [
    "Or use the flax api (which is going to be necessary once our model gets more complicated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f1de06e-33b0-4502-9a7b-a956ce7c0c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.5195726 ,  0.2808921 ],\n",
       "       [-0.5195726 ,  0.2808921 ],\n",
       "       [-0.9512113 ,  0.35392332],\n",
       "       [-1.8304102 , -0.332351  ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, num2tokens(2234))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171978fd-9d04-4be9-9572-7aacd3a5e607",
   "metadata": {},
   "source": [
    "# Sequence masking\n",
    "Transformers have a fixed context window. This means there's a maximum sequence length and every sequence passed to the model must be of that length. We handle this issue in two ways:\n",
    "1. padding\n",
    "2. masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f3d42-f2eb-4800-8be4-d5d2fe42b3fc",
   "metadata": {},
   "source": [
    "## Padding\n",
    "Let's say our max sequence length is 5 and we're constructing a training batch from the following 4 numbers: `[123, 42451, 0, 12]`. Our sequences look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65e06157-1062-4c39-a2c8-5cfd24e83515",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Array([1, 2, 3], dtype=int32),\n",
       " Array([4, 2, 4, 5, 1], dtype=int32),\n",
       " Array([0], dtype=int32),\n",
       " Array([1, 2], dtype=int32)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[num2tokens(x) for x in [123, 42451, 0, 12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c5a508-c51f-4b49-9db3-c2063cb97632",
   "metadata": {},
   "source": [
    "They're all of different lengths so we can't put them into a matrix, and we can't feed them to our model. We need to pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6feabab-f59c-49cb-a877-d74b7f2975bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1,  2,  3, 10, 10],\n",
       "       [ 4,  2,  4,  5,  1],\n",
       "       [ 0, 10, 10, 10, 10],\n",
       "       [ 1,  2, 10, 10, 10]], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [num2tokens(x) for x in [123, 42451, 0, 12]]\n",
    "tokens = jnp.array([jnp.pad(x, (0, 5 - len(x)), 'constant', constant_values=(0, vocab['p']))\n",
    "                    for x in tokens])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e745d78-3674-4c11-ba51-aae506f6ee17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 5, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, tokens).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa8d9ed-dc73-468a-aa37-a764e66dc0f0",
   "metadata": {},
   "source": [
    "## Masking\n",
    "Now we can fit these sequences into a batch, but we don't want the model to learn from the padded characters. So for that, we [mask](https://www.ml-science.com/masking). The idea being that we identify which tokens we don't want feeding into gradients so that we can later ensure they don't. There are several ways to do it. Here's one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aea23d25-0105-45ce-8979-7dfb04e5d0f1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = (tokens == vocab['p']).astype(jnp.float32)\n",
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc278e4c-f9a8-4403-949c-ee2f9b95d8b0",
   "metadata": {},
   "source": [
    "More on this in a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d87fb80-4fb1-4e3c-955d-931fee68f5d1",
   "metadata": {},
   "source": [
    "# Positional encoding\n",
    "\n",
    "![image.png](../_static/post/another-annotated-transformer/posenc-circled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23ac219-15e2-42d3-88a3-6c42bff07571",
   "metadata": {},
   "source": [
    "The positional encoding is the same size as a single observation fed to the model and added to each observation in the batch. We use the same function as they used in the original paper. Let $X\\in\\mathbb{R}^{s\\times d}$ where $s$ is the max sequence length, and $d$ is the embedding dimension.\n",
    "\n",
    "$$f(X_{i,j}) = \\begin{cases}\n",
    "\\sin\\left(i/\\left(10000^{j/d}\\right)\\right) & \\text{if } j\\equiv 0\\pmod{2} \\\\\n",
    "\\cos\\left(i/\\left(10000^{(j-1)/d}\\right)\\right) & \\text{if } j\\equiv 1\\pmod{2}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bba0a38-9ce1-4ebe-9a34-ab88519a5c85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.        ,  1.        ],\n",
       "       [ 0.84147096,  0.5403023 ],\n",
       "       [ 0.9092974 , -0.41614684],\n",
       "       [ 0.14112   , -0.9899925 ],\n",
       "       [-0.7568025 , -0.6536436 ]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sin_pos_enc(sequence_length, embed_dim):\n",
    "    \"\"\"create sin/cos positional encodings\n",
    "\n",
    "    Paramters\n",
    "    =========\n",
    "    sequence_length : int\n",
    "        The max length of the input sequences for this model\n",
    "    embed_dim : int\n",
    "        the embedding dimension\n",
    "\n",
    "    Returns\n",
    "    =======\n",
    "    a matrix of shape: (sequence_length, embed_dim)\n",
    "    \"\"\"\n",
    "    chex.assert_is_divisible(embed_dim, 2)\n",
    "    X = jnp.expand_dims(jnp.arange(sequence_length), 1) / \\\n",
    "        jnp.power(10000, jnp.arange(embed_dim, step=2) / embed_dim)\n",
    "    out = jnp.empty((sequence_length, embed_dim))\n",
    "    out = out.at[:, 0::2].set(jnp.sin(X))\n",
    "    out = out.at[:, 1::2].set(jnp.cos(X))\n",
    "    return out\n",
    "\n",
    "sin_pos_enc(5, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac59a77-27de-4a83-bcdd-7a7aa7ec7e3d",
   "metadata": {},
   "source": [
    "So when we implement the `Encoder` and the `Decoder`, we'll sum the embeddings and the positional encodings (in practice, we scale the embeddings beforehand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "723f2379-e41c-4b6f-a548-b0ccfe2dc655",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.0000000e+00,  3.1376272e-01],\n",
       "       [ 3.4107921e-01,  4.8709157e-01],\n",
       "       [-4.7244602e-01, -1.1689236e-01],\n",
       "       [-1.3423494e-01, -3.5038143e-01],\n",
       "       [ 1.3852590e+00,  2.1723911e-01],\n",
       "       [-3.8399076e-01,  2.3916358e-02],\n",
       "       [ 1.2069740e-01, -7.5244367e-01],\n",
       "       [ 3.5928103e-01,  2.6903003e-01],\n",
       "       [ 7.1149126e-02,  2.5695641e-02],\n",
       "       [-2.0101637e-01, -3.0685776e-01],\n",
       "       [-9.2026454e-01, -5.6686229e-01],\n",
       "       [-7.0798770e-02,  2.4459933e-04]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['params']['embedding'] * sin_pos_enc(*params['params']['embedding'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b0cc6-dc71-49c2-b00b-9d6eea6f1f08",
   "metadata": {},
   "source": [
    "# Attention\n",
    "![image.png](../_static/post/another-annotated-transformer/attn-circled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8155ec59-f548-4dd9-927b-a167ea8f7f6d",
   "metadata": {},
   "source": [
    "Transformers are built around this **attention** mechanism, so this warrants its own section. Multi-head attention involves stacking a collection of attention \"heads\" and adding some learned weights in the mix. As such, we'll start with attention heads and progress to multi-head attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a0913-9f82-43da-8bf7-4956f2554e1e",
   "metadata": {},
   "source": [
    "Attention is just a function that takes 3 arguments (key, value, and query) and aggregates them to a vector. There are a few forms of attention but we'll focus on the one used in the seminal paper: **scaled dot product** attention."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe178f2e-9be1-4c14-bcaf-4b133228fe22",
   "metadata": {},
   "source": [
    "## Scaled dot product attention\n",
    "\n",
    "Let $Q\\in\\mathbb{R}^{n\\times d},K\\in\\mathbb{R}^{m\\times d},V\\in\\mathbb{R}^{m\\times v}$ be the **query**, **key**, and **value**. Basically we just need the shapes to be fit for the matrix multiplication below. A good reference for this is [d2l.ai](https://d2l.ai/chapter_attention-mechanisms-and-transformers/attention-scoring-functions.html).\n",
    "\n",
    "$$\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)V\\in\\mathbb{R}^{n\\times v}$$\n",
    "\n",
    "The $\\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d}}\\right)$ part is called the **attention weights**.\n",
    "\n",
    "It's worthwhile to note that there are no learnable weights in this formula.\n",
    "\n",
    "This formula is deceptive in 2 ways:\n",
    "1. The softmax is actually a masked softmax\n",
    "2. There's generally some dropout on the attention weights\n",
    "\n",
    "Let's take this piece by piece."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea95e2-1b0f-4340-a397-500c3a59373a",
   "metadata": {},
   "source": [
    "### Masked softmax\n",
    "\n",
    "Let $X\\in\\mathbb{R}^k$ a vector, then $\\text{softmax}(X)\\in\\mathbb{R}^k$.\n",
    "\n",
    "$$\\text{softmax}(X)_i = \\frac{e^{X_i}}{\\sum_{j=0}^{k-1}e^{X_j}}$$\n",
    "\n",
    "It's just normalization with a monotonic function applied, meaning the relative ranking of the elements of $X$ aren't changed. For more on this, see [this](https://charlielehman.github.io/post/visualizing-tempscaling/) post.\n",
    "\n",
    "For masked softmax, we'll be taking the approximate approach. Because of the sum in the denominator and the exponentiation, it's unwise to mask with 0 ($e^0 = 1$). Instead we'll mask with a very large negative number before we exponentiate so that the result is close to 0 ($e^{-\\infty} \\approx 0$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faa5f22-7715-4852-894c-408cbc22daf0",
   "metadata": {},
   "source": [
    "Let's recall our tokens and mask from earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62ca66b8-2456-4e14-a840-acb2c1b86c2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1,  2,  3, 10, 10],\n",
       "       [ 4,  2,  4,  5,  1],\n",
       "       [ 0, 10, 10, 10, 10],\n",
       "       [ 1,  2, 10, 10, 10]], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ea1ba21-b3dc-4e08-967c-f8daea83cb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 0.],\n",
       "       [0., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff58edf-dad5-4045-9a06-19cca4c06426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[0.09003057, 0.24472846, 0.6652409 , 0.        , 0.        ],\n",
       "       [0.20393994, 0.02760027, 0.20393994, 0.55436623, 0.01015357],\n",
       "       [1.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.26894143, 0.7310586 , 0.        , 0.        , 0.        ]],      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def masked_softmax(args, mask):\n",
    "    if mask is not None:\n",
    "        args = args + (mask.astype(args.dtype) * -10_000.0)\n",
    "    return nn.softmax(args)\n",
    "\n",
    "masked_softmax(tokens, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268eb13-1476-4f31-934d-000363f7d095",
   "metadata": {},
   "source": [
    "### The meat\n",
    "\n",
    "Let's start with a few simplifications and then add pieces in.\n",
    "\n",
    "1. assume that $K=V$,\n",
    "2. assume the rows of $V$ all have unit norm\n",
    "3. ignore the softmax and the $1/\\sqrt{d}$ factor.\n",
    "    \n",
    "Under these assumptions, the attention weights become $(QV^T)V$, which is the [projection](https://en.wikipedia.org/wiki/Vector_projection) of $Q$ onto the rows of $V$. Things are pretty interpretable under those simplifying assumptions, so let's start justifying them.\n",
    "\n",
    "1. Although it's not true that $K=V$ in general, it is true when it comes to transformers. You can see in the diagram that the key and value (although not labeled) are always the same in all attention heads.\n",
    "2. We have normalization layers after every step, so this shouldn't be too off.\n",
    "3. Although this does break the interpretation, it's not so bad still because it's at least a monotonically increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08167598-e27e-4041-9e6b-61256d293432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 11)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = jran.normal(key, (3, 7))\n",
    "K = jran.normal(key, (5, 7))\n",
    "V = jran.normal(key, (5, 11))\n",
    "(masked_softmax(Q @ K.T / jnp.sqrt(Q.shape[-1]), None) @ V).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f75de64-78a0-4874-84ef-80704f1c1a64",
   "metadata": {},
   "source": [
    "When we add a batch dimension (and later a `num_heads` dimension) we'll want to broadcast the multiplication over those dimensions, so we won't be able to use `K.T`. Instead, we'll use `K.swapaxes`, which will allow us to swap only the last two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3b79051-2620-4ee4-98a0-efc0f9057a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 3, 11)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = jran.normal(key, (11, 3, 7))\n",
    "K = jran.normal(key, (11, 5, 7))\n",
    "V = jran.normal(key, (11, 5, 11))\n",
    "(masked_softmax(Q @ K.swapaxes(-1, -2) / jnp.sqrt(Q.shape[-1]), None) @ V).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e171ec-4c84-42c1-841d-da9e972c4f3c",
   "metadata": {},
   "source": [
    "Bringing it all together and adding some dropout to the attention weights, we end up with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "645db686-aab2-4c51-a94c-76a04fb1db90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dot_prod_attn(q, k, v, dropout=lambda x: x, mask=None):\n",
    "    # NxD @ DxM => NxM\n",
    "    # (B[, H], N, M)\n",
    "    attn_scores = q @ k.swapaxes(-2, -1) / jnp.sqrt(q.shape[-1])\n",
    "    attn_weights = masked_softmax(attn_scores, mask)\n",
    "    # (B[, H], N, D)\n",
    "    out = dropout(attn_weights) @ v\n",
    "    return out, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71b9ff0-29c2-4d31-8529-68633a5f4376",
   "metadata": {},
   "source": [
    "## Multihead Attention\n",
    "\n",
    "![image.png](../_static/post/another-annotated-transformer/multi-head-attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591e7447-011c-4937-8b5c-c9faa22b0c80",
   "metadata": {},
   "source": [
    "At a high level, mutli-head attention is a bunch of stacked attention layers. But given that there are no learnable weights in the attention heads (they query, key, and values are all arguments), each would yield the same result -- not so useful. So instead, we train a linear layer per attention head, and then combine the results.\n",
    "\n",
    "Our interpretation of attention being the projection needs some adjustment now that we're applying these linear layers (it's no longer true that $K=V$). But we'll leave this for the reader (or maybe another post)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aadf85b-2d19-4874-b1fe-17647ebb82dc",
   "metadata": {},
   "source": [
    "### One linear vs stacked linears"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab7cb12-a778-44e9-b572-d0a76c5bafb9",
   "metadata": {},
   "source": [
    "In practice, most implementations use one linear layer and reshape the output rather than storing a collection of linear models. At first that might not seem kosher, but it is. Here's a visual interpretation.\n",
    "\n",
    "![stacked-linears](../_static/post/another-annotated-transformer/nn.graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62ff57-67d1-450d-99cf-a28928dd618b",
   "metadata": {},
   "source": [
    "To show this in action, we'll approach this both ways. We'll first instantiate a dense layer much like the full diagram above, then we'll split out the red and blue arrows, and compare the two outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5527da1-f14a-49f5-88a5-287ff471d58f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "sequence_length = 5\n",
    "embed_dim = 3\n",
    "n_heads = 2\n",
    "size_per_head = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b002a46-b2ac-40df-8597-7853c6be7370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = jnp.arange(batch_size * sequence_length * embed_dim)\n",
    "X = X.reshape((batch_size, sequence_length, embed_dim))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "310beb55-d4e9-4d2c-a4c0-babdaf284656",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        kernel: Array([[ 0.4087802 ,  0.43891278, -0.23872387, -0.8494273 ],\n",
       "               [ 0.41122693, -0.5888459 , -0.55229884,  0.49776074],\n",
       "               [ 0.3480036 , -0.7046275 , -0.30813402, -1.21659   ]],      dtype=float32),\n",
       "        bias: Array([0., 0., 0., 0.], dtype=float32),\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = nn.Dense(n_heads * size_per_head).init(key, X)\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea6fab43-4e13-40d6-a9df-f5e2c7c5ffcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_list = []\n",
    "for i in range(n_heads):\n",
    "    p = params.unfreeze()\n",
    "    p['params'] = {\n",
    "        'kernel': p['params']['kernel'][:, i * size_per_head:(i + 1) * size_per_head],\n",
    "        'bias': p['params']['bias'][i * size_per_head:(i + 1) * size_per_head]\n",
    "    }\n",
    "    param_list.append(core.freeze(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18384aec-82c1-46e5-9547-5cd8a2449f7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[  1.1072341,  -1.998101 ],\n",
       "        [  4.611266 ,  -4.561783 ],\n",
       "        [  8.115298 ,  -7.1254644],\n",
       "        [ 11.61933  ,  -9.689147 ],\n",
       "        [ 15.123363 , -12.252829 ]],\n",
       "\n",
       "       [[ -1.168567 ,  -1.9354193],\n",
       "        [ -4.466037 ,  -6.640189 ],\n",
       "        [ -7.7635074, -11.344959 ],\n",
       "        [-11.060977 , -16.049728 ],\n",
       "        [-14.358448 , -20.7545   ]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Array([[[  1.1072341,  -1.998101 ],\n",
       "        [  4.611266 ,  -4.561783 ],\n",
       "        [  8.115298 ,  -7.1254644],\n",
       "        [ 11.61933  ,  -9.689147 ],\n",
       "        [ 15.123363 , -12.252829 ]],\n",
       "\n",
       "       [[ -1.168567 ,  -1.9354193],\n",
       "        [ -4.466037 ,  -6.640189 ],\n",
       "        [ -7.7635074, -11.344959 ],\n",
       "        [-11.060977 , -16.049728 ],\n",
       "        [-14.358448 , -20.7545   ]]], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(jnp.stack([nn.Dense(size_per_head).apply(p, X) for p in param_list]).swapaxes(0, 1)[0])\n",
    "print('=' * 50)\n",
    "display(nn.Dense(n_heads * size_per_head).apply(params, X)\\\n",
    "        .reshape((batch_size, sequence_length, n_heads, size_per_head))\\\n",
    "        .swapaxes(1, 2)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a12cfa-ed65-4340-ae5f-bb7aecd99b5a",
   "metadata": {},
   "source": [
    "We're ready to implement `Multi-Head Attention` layer. You'll notice there's some weird stuff going on with the mask. This is intentional and it'll make more sense once we implement the `DecoderLayer`[^mha_mask].\n",
    "\n",
    "[^mha_mask]: You have OCD, huh? Yeah, me too... If check out the diagram, notice the `Masked Multi-Head Attention` layer. We'll be masking the attention weights for the self attention layer. That mask is of a different shape from the sequence masking, so these counterintuitive lines dealing with the mask are to facilitate that later. It seemed easier than having to edit the classes and whatnot just for that one issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1579249-a08a-470e-a1c5-aabad4756fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    n_heads: int\n",
    "    size_per_head: int\n",
    "    attn_dropout: float\n",
    "    fc_dropout: float\n",
    "    attn_fn: Callable = dot_prod_attn\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, q, k, v, mask=None, training=False):\n",
    "        \"expected shape: Batch, [N|M], Dim\"\n",
    "        B, N, D = q.shape\n",
    "        _, M, _ = k.shape\n",
    "\n",
    "        def qkv_layer(x, name):\n",
    "            x = nn.Dense(self.n_heads * self.size_per_head, name=name)(x)\n",
    "            x = x.reshape((B, -1, self.n_heads, self.size_per_head)).swapaxes(1, 2)\n",
    "            return x\n",
    "        # BxNxD => BxHxNxP\n",
    "        q = qkv_layer(q, 'query_linear')\n",
    "        # BxMxD => BxHxMxP\n",
    "        k = qkv_layer(k, 'key_linear')\n",
    "        # BxMxD => BxHxMxP\n",
    "        v = qkv_layer(v, 'value_linear')\n",
    "        if mask is not None:\n",
    "            # accounting for reshape in qkv_layer\n",
    "            # B[xN]xN   => Bx1[xN]xN\n",
    "            mask = jnp.expand_dims(mask, 1)\n",
    "            if mask.ndim < q.ndim:\n",
    "                # softmax is applied to dim -1\n",
    "                # Bx1xN => Bx1x1xN\n",
    "                mask = jnp.expand_dims(mask, -2)\n",
    "        attn_do = nn.Dropout(self.attn_dropout, deterministic=not training, name='attn_dropout')\n",
    "        out, attn_weights = self.attn_fn(q, k, v, attn_do, mask=mask)\n",
    "        # uncomment to keep attention weights in state\n",
    "        # self.sow('intermediates', 'weights', attn_weights)\n",
    "        out = out.swapaxes(1, 2).reshape((B, N, -1))\n",
    "        out = nn.Dense(D, name='output_linear')(out)\n",
    "        out = nn.Dropout(self.fc_dropout, deterministic=not training, name='fc_dropout')(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fede895-1cb0-4022-b319-838021c58530",
   "metadata": {},
   "source": [
    "To better understand this model, we'll calculate the number of parameters and then see if we're right.\n",
    "\n",
    "Each of the `query`, `key`, and `value` linear layers has `embed_dim * n_heads * size_per_head + size_per_head` many parameters (the kernel and the bias terms). The final linear layer brings us back to `embed_dim` size, so we have `n_heads * size_per_head * embed_dim + embed_dim`.\n",
    "\n",
    "All together, our formula is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d642f4e-2280-4ac0-85a3-8afefa6ee8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3 * (n_heads * (size_per_head * embed_dim + size_per_head)) + (n_heads * size_per_head * embed_dim + embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdc0abc-f35f-4ea8-a896-f4f58e90ba4b",
   "metadata": {},
   "source": [
    "Let's check our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6c8fe1b-856e-43be-94b4-0aa1d9e7913f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_heads, size_per_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51aafe6e-8de7-493b-b950-4d1069c30e42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = jran.uniform(key, (batch_size, sequence_length, embed_dim))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d135197-c342-47a7-a901-035f4bac7b13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FrozenDict({\n",
       "    params: {\n",
       "        key_linear: {\n",
       "            bias: (4,),\n",
       "            kernel: (3, 4),\n",
       "        },\n",
       "        output_linear: {\n",
       "            bias: (3,),\n",
       "            kernel: (4, 3),\n",
       "        },\n",
       "        query_linear: {\n",
       "            bias: (4,),\n",
       "            kernel: (3, 4),\n",
       "        },\n",
       "        value_linear: {\n",
       "            bias: (4,),\n",
       "            kernel: (3, 4),\n",
       "        },\n",
       "    },\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdl = MultiHeadAttention(n_heads, size_per_head, attn_dropout=0.2, fc_dropout=0.3)\n",
    "params = mdl.init(key, X, X, X, mask=(jnp.max(X, axis=-1) < 0.8).astype(jnp.float32))\n",
    "\n",
    "jax.tree_map(jnp.shape, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "721b9041-b8bd-4c23-9773-c6ce785d92dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                    MultiHeadAttention Summary                                     </span>\n",
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> path          </span>┃<span style=\"font-weight: bold\"> module             </span>┃<span style=\"font-weight: bold\"> inputs           </span>┃<span style=\"font-weight: bold\"> outputs          </span>┃<span style=\"font-weight: bold\"> params               </span>┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│               │ MultiHeadAttention │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,3] │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,3]   │                      │\n",
       "│               │                    │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,3] │                  │                      │\n",
       "│               │                    │ - <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,3] │                  │                      │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│ query_linear  │ Dense              │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,3]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,4]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4]     │\n",
       "│               │                    │                  │                  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,4] │\n",
       "│               │                    │                  │                  │                      │\n",
       "│               │                    │                  │                  │ <span style=\"font-weight: bold\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(64 B)</span>            │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│ key_linear    │ Dense              │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,3]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,4]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4]     │\n",
       "│               │                    │                  │                  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,4] │\n",
       "│               │                    │                  │                  │                      │\n",
       "│               │                    │                  │                  │ <span style=\"font-weight: bold\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(64 B)</span>            │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│ value_linear  │ Dense              │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,3]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,4]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4]     │\n",
       "│               │                    │                  │                  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3,4] │\n",
       "│               │                    │                  │                  │                      │\n",
       "│               │                    │                  │                  │ <span style=\"font-weight: bold\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(64 B)</span>            │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│ attn_dropout  │ Dropout            │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,2,5,5] │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,2,5,5] │                      │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│ output_linear │ Dense              │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,4]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,3]   │ bias: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[3]     │\n",
       "│               │                    │                  │                  │ kernel: <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[4,3] │\n",
       "│               │                    │                  │                  │                      │\n",
       "│               │                    │                  │                  │ <span style=\"font-weight: bold\">15 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(60 B)</span>            │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│ fc_dropout    │ Dropout            │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,3]   │ <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">float32</span>[2,5,3]   │                      │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│<span style=\"font-weight: bold\">               </span>│<span style=\"font-weight: bold\">                    </span>│<span style=\"font-weight: bold\">                  </span>│<span style=\"font-weight: bold\">            Total </span>│<span style=\"font-weight: bold\"> 63 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(252 B)</span><span style=\"font-weight: bold\">           </span>│\n",
       "└───────────────┴────────────────────┴──────────────────┴──────────────────┴──────────────────────┘\n",
       "<span style=\"font-weight: bold\">                                                                                                   </span>\n",
       "<span style=\"font-weight: bold\">                                   Total Parameters: 63 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">(252 B)</span><span style=\"font-weight: bold\">                                    </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3m                                    MultiHeadAttention Summary                                     \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mpath         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mmodule            \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1minputs          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1moutputs         \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mparams              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│               │ MultiHeadAttention │ - \u001b[2mfloat32\u001b[0m[2,5,3] │ \u001b[2mfloat32\u001b[0m[2,5,3]   │                      │\n",
       "│               │                    │ - \u001b[2mfloat32\u001b[0m[2,5,3] │                  │                      │\n",
       "│               │                    │ - \u001b[2mfloat32\u001b[0m[2,5,3] │                  │                      │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│ query_linear  │ Dense              │ \u001b[2mfloat32\u001b[0m[2,5,3]   │ \u001b[2mfloat32\u001b[0m[2,5,4]   │ bias: \u001b[2mfloat32\u001b[0m[4]     │\n",
       "│               │                    │                  │                  │ kernel: \u001b[2mfloat32\u001b[0m[3,4] │\n",
       "│               │                    │                  │                  │                      │\n",
       "│               │                    │                  │                  │ \u001b[1m16 \u001b[0m\u001b[1;2m(64 B)\u001b[0m            │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│ key_linear    │ Dense              │ \u001b[2mfloat32\u001b[0m[2,5,3]   │ \u001b[2mfloat32\u001b[0m[2,5,4]   │ bias: \u001b[2mfloat32\u001b[0m[4]     │\n",
       "│               │                    │                  │                  │ kernel: \u001b[2mfloat32\u001b[0m[3,4] │\n",
       "│               │                    │                  │                  │                      │\n",
       "│               │                    │                  │                  │ \u001b[1m16 \u001b[0m\u001b[1;2m(64 B)\u001b[0m            │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│ value_linear  │ Dense              │ \u001b[2mfloat32\u001b[0m[2,5,3]   │ \u001b[2mfloat32\u001b[0m[2,5,4]   │ bias: \u001b[2mfloat32\u001b[0m[4]     │\n",
       "│               │                    │                  │                  │ kernel: \u001b[2mfloat32\u001b[0m[3,4] │\n",
       "│               │                    │                  │                  │                      │\n",
       "│               │                    │                  │                  │ \u001b[1m16 \u001b[0m\u001b[1;2m(64 B)\u001b[0m            │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│ attn_dropout  │ Dropout            │ \u001b[2mfloat32\u001b[0m[2,2,5,5] │ \u001b[2mfloat32\u001b[0m[2,2,5,5] │                      │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│ output_linear │ Dense              │ \u001b[2mfloat32\u001b[0m[2,5,4]   │ \u001b[2mfloat32\u001b[0m[2,5,3]   │ bias: \u001b[2mfloat32\u001b[0m[3]     │\n",
       "│               │                    │                  │                  │ kernel: \u001b[2mfloat32\u001b[0m[4,3] │\n",
       "│               │                    │                  │                  │                      │\n",
       "│               │                    │                  │                  │ \u001b[1m15 \u001b[0m\u001b[1;2m(60 B)\u001b[0m            │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│ fc_dropout    │ Dropout            │ \u001b[2mfloat32\u001b[0m[2,5,3]   │ \u001b[2mfloat32\u001b[0m[2,5,3]   │                      │\n",
       "├───────────────┼────────────────────┼──────────────────┼──────────────────┼──────────────────────┤\n",
       "│\u001b[1m \u001b[0m\u001b[1m             \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                  \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m                \u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m           Total\u001b[0m\u001b[1m \u001b[0m│\u001b[1m \u001b[0m\u001b[1m63 \u001b[0m\u001b[1;2m(252 B)\u001b[0m\u001b[1m          \u001b[0m\u001b[1m \u001b[0m│\n",
       "└───────────────┴────────────────────┴──────────────────┴──────────────────┴──────────────────────┘\n",
       "\u001b[1m                                                                                                   \u001b[0m\n",
       "\u001b[1m                                   Total Parameters: 63 \u001b[0m\u001b[1;2m(252 B)\u001b[0m\u001b[1m                                    \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn.tabulate(mdl, key, console_kwargs=dict(force_jupyter=True))(X, X, X);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3436d2-6f08-4eb2-be42-32c475cb844e",
   "metadata": {},
   "source": [
    "Since we'll want to see this a few times, let's write a function for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f42d628d-7415-40a9-ae19-1be7cec92ad5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def num_params(params):\n",
    "    return jnp.sum(jnp.array(jax.tree_util.tree_flatten(jax.tree_map(lambda x: jnp.prod(jnp.array(jnp.shape(x))), params))[0])).item()\n",
    "\n",
    "num_params(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a46b05a-c9bf-4933-9775-c9ea231811c9",
   "metadata": {},
   "source": [
    "# Add & Norm\n",
    "\n",
    "We'll implement an `AddAndNorm` layer just so our code looks like the diagram. The layer is so simple that you're likely to see implementations that don't implement this and just do it in the `EncoderLayer` or `DecoderLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bff2846e-a8bf-41ab-98ed-b589fbe36514",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AddAndNorm(nn.Module):\n",
    "    \"\"\"The add and norm.\"\"\"\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, X, X_out):\n",
    "        return nn.LayerNorm()(X + X_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16caa2f4-fee0-447c-98b8-3d78f0ab298a",
   "metadata": {},
   "source": [
    "# Feed forward\n",
    "\n",
    "Same deal as `AddAndNorm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6cf3abb5-9737-44a2-a3fc-603938c81f4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"\"\"a 2-layer feed-forward network.\"\"\"\n",
    "    hidden_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, X):\n",
    "        D = X.shape[-1]\n",
    "        X = nn.Dense(self.hidden_dim)(X)\n",
    "        X = nn.relu(X)\n",
    "        X = nn.Dense(D)(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81751f41-7f01-41a1-b373-7e6093330d4f",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4287d9-8435-4709-8227-6b788f10c5c1",
   "metadata": {},
   "source": [
    "## EncoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d7bd40-70ec-4c08-8e83-f2ededb609d5",
   "metadata": {},
   "source": [
    "![image.png](../_static/post/another-annotated-transformer/encoder-layer-circled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e8e68d-b267-40a5-bfcb-9f093e22b7a1",
   "metadata": {},
   "source": [
    "The `Encoder` is a combination of the various layers we've already built up along with several `EncoderLayer`s (which are themselves just combinations of previously defined layers). This section is going to be short.\n",
    "\n",
    "Note the `EncoderLayer` takes one argument (neglecting the mask) and feeds that one argument as the `query`, `key`, and `value` in the `Multi-Head Attention` layer. This can be seen by following the arrows in the diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a72d8004-b9ee-4fc5-bcd1-40b4f5cb91d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    hidden_dim: int\n",
    "    n_heads: int\n",
    "    size_per_head: int\n",
    "    attn_dropout: float\n",
    "    fc_dropout: float\n",
    "\n",
    "    def setup(self):\n",
    "        self.attn = MultiHeadAttention(n_heads=self.n_heads,\n",
    "                                       size_per_head=self.size_per_head,\n",
    "                                       attn_dropout=self.attn_dropout,\n",
    "                                       fc_dropout=self.fc_dropout)\n",
    "        self.aan_0 = AddAndNorm()\n",
    "        self.ff = FeedForward(hidden_dim=self.hidden_dim)\n",
    "        self.aan_1 = AddAndNorm()\n",
    "\n",
    "    def __call__(self, X, mask=None, training=False):\n",
    "        X1 = self.attn(X, X, X, mask=mask, training=training)\n",
    "        X = self.aan_0(X, X1)\n",
    "        X1 = self.ff(X)\n",
    "        X = self.aan_1(X, X1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb187a7-84fb-42aa-983e-202fd0bde6fc",
   "metadata": {},
   "source": [
    "![image.png](../_static/post/another-annotated-transformer/encoder-circled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "111771ff-62f5-41fa-91ac-6d242ee96f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    pos_encoding: Callable[[int, int], jnp.array]\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "    layers: Sequence[EncoderLayer]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, X, mask=None, training=False):\n",
    "        B, N = X.shape\n",
    "        if mask is not None:\n",
    "            chex.assert_shape(mask, (B, N))\n",
    "        X = nn.Embed(self.vocab_size, self.embed_dim, name='embed')(X)\n",
    "        X = X * jnp.sqrt(self.embed_dim)\n",
    "        # X.shape[-2] is the sequence length\n",
    "        X = X + self.pos_encoding(X.shape[-2], self.embed_dim)\n",
    "        for layer in self.layers:\n",
    "            X = layer(X, mask=mask, training=training)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518c5b5b-4fbc-45ea-a779-088f9478de07",
   "metadata": {},
   "source": [
    "Just for fun, let's check out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f065dcd8-cfe6-4231-80e5-04d2beaaed3a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6, 30)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer_fn():\n",
    "    return EncoderLayer(hidden_dim=13,\n",
    "                        attn_dropout=0.1,\n",
    "                        fc_dropout=0.1,\n",
    "                        n_heads=7,\n",
    "                        size_per_head=17)\n",
    "mdl = Encoder(pos_encoding=sin_pos_enc, vocab_size=len(vocab),\n",
    "              embed_dim=2 * 3 * 5,\n",
    "              layers=[layer_fn() for _ in range(3)])\n",
    "batch = [num2tokens(x) for x in jran.randint(key, (3,), 0, 1e5)]\n",
    "batch = jnp.stack([jnp.pad(x, (0, 6 - len(x)), 'constant', constant_values=vocab['p']) for x in batch])\n",
    "mask = (batch == vocab['p'])\n",
    "params = mdl.init(key, batch)\n",
    "resp = mdl.apply(params, batch, mask=mask, training=True, rngs={'dropout': key})\n",
    "resp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0947c299-a015-4a4b-8d80-b4fd0a081153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47190"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params(params['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed40eae-819c-42dc-9de1-3f0ed1521696",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "There are a subset of transformers called `encoder-only` transformers. Now you know what that means. They take in a sequence of tokens, train the non-contextual embeddings (the `Embedding` layer) and output contextual embeddings. The output embeddings are a function of the word itself, but also the context that the word appeared in.\n",
    "\n",
    "The quintessential example of an encoder-only transformer is [Bert](https://arxiv.org/pdf/1810.04805.pdf) from Google research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a3c4ad-fda4-4924-b847-165a9996639c",
   "metadata": {},
   "source": [
    "# Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83f417c-4e35-4a9d-887e-29831ffc9fb3",
   "metadata": {},
   "source": [
    "## DecoderLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f720a81-4d68-4b6f-af66-9fe7b996712f",
   "metadata": {},
   "source": [
    "![image.png](../_static/post/another-annotated-transformer/decoder-layer-circled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942688fa-4c6e-4084-87cf-dd60cddad90b",
   "metadata": {},
   "source": [
    "There's one last piece to implement: The `Masked Multi-Head Attention`. We've implemented regular `Multi-Head Attention`, but not the masked part. The idea with the masked attention is to feed the whole sequence in at once, but still train the model as if we hadn't. To that end, we restrict the one place in the `Decoder` where information is shared across elements of the output sequence (the self-attention layer) so that a given position can only use information from previous positions. This is commonly described as restricting the positions that a given output can attend to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a2da62e-9897-4108-92e2-00b78ba63b86",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[[False,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True],\n",
       "        [False, False, False,  True,  True],\n",
       "        [False, False, False, False,  True],\n",
       "        [False, False, False, False, False]]], dtype=bool)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def causal_mask(shape):\n",
    "    return jnp.triu(jnp.ones(shape, dtype=jnp.bool_), k=1)\n",
    "\n",
    "causal_mask((1, 5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3172218a-c3e8-47d4-8f20-ea5a6d1063c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    hidden_dim: int\n",
    "    n_heads: int\n",
    "    size_per_head: int\n",
    "    attn_dropout: float\n",
    "    fc_dropout: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, X_enc, X_dec, enc_mask, dec_mask, training=False):\n",
    "\n",
    "        def attn(q, kv, mask, training, name):\n",
    "            mdl = MultiHeadAttention(n_heads=self.n_heads,\n",
    "                                     size_per_head=self.size_per_head,\n",
    "                                     attn_dropout=self.attn_dropout,\n",
    "                                     fc_dropout=self.fc_dropout,\n",
    "                                     name=f'{name}_attn')\n",
    "            out = mdl(q, kv, kv, mask=mask, training=training)\n",
    "            aan = AddAndNorm(name=f'{name}_addnorm')\n",
    "            return aan(q, out)\n",
    "        X_dec = attn(X_dec, X_dec, dec_mask, training, 'self')\n",
    "        X_dec = attn(X_dec, X_enc, enc_mask, training, 'src')\n",
    "        X1 = FeedForward(hidden_dim=self.hidden_dim)(X_dec)\n",
    "        X_dec = AddAndNorm()(X_dec, X1)\n",
    "        return X_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cf8a85-5e19-4365-92c0-585c8de26271",
   "metadata": {},
   "source": [
    "![image.png](../_static/post/another-annotated-transformer/decoder-circled.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e9691db-a6fd-4a32-84ed-857b18c50c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    pos_encoding: Callable[[int, int], jnp.array]\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "    layers: Sequence[DecoderLayer]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, X_enc, X_dec, enc_mask, training=False):\n",
    "        B, N = X_dec.shape[:2]\n",
    "        dec_mask = causal_mask((1, N, N))\n",
    "        X_dec = nn.Embed(self.vocab_size, self.embed_dim, name='embed')(X_dec)\n",
    "        X_dec = X_dec * jnp.sqrt(self.embed_dim)\n",
    "        # X.shape[-2] is the sequence length\n",
    "        X_dec = X_dec + self.pos_encoding(X_dec.shape[-2], self.embed_dim)\n",
    "        for layer in self.layers:\n",
    "            X_dec = layer(X_enc, X_dec, enc_mask, dec_mask, training=training)\n",
    "        X_dec = nn.Dense(self.vocab_size, name='final')(X_dec)\n",
    "        return X_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65239805-a3be-4e46-9695-3bce2a5d2ad0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 6, 12)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def layer_fn():\n",
    "    return DecoderLayer(hidden_dim=13,\n",
    "                        attn_dropout=0.1,\n",
    "                        fc_dropout=0.1,\n",
    "                        n_heads=7,\n",
    "                        size_per_head=17)\n",
    "mdl = Decoder(pos_encoding=sin_pos_enc,\n",
    "              vocab_size=len(vocab),\n",
    "              embed_dim=2 * 3 * 5,\n",
    "              layers=[layer_fn() for _ in range(3)])\n",
    "batch = [num2tokens(x) for x in jran.randint(key, (3,), 0, 1e5)]\n",
    "batch = jnp.stack([jnp.pad(x, (0, 6 - len(x)), 'constant', constant_values=vocab['p']) for x in batch])\n",
    "kv = [num2tokens(x) for x in jran.randint(key + 1, (3,), 0, 1e6)]\n",
    "kv = jnp.stack([jnp.pad(x, (0, 6 - len(x)), 'constant', constant_values=vocab['p']) for x in kv])\n",
    "enc_mask = (kv == vocab['p'])\n",
    "kv = nn.one_hot(kv, len(vocab))\n",
    "params = mdl.init(key, kv, batch, enc_mask)\n",
    "resp = mdl.apply(params, kv, batch, enc_mask, training=True, rngs={'dropout': key})\n",
    "resp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bb80c47-d22d-49c9-bbc1-1c5c68623f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78891"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_params(params['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a605296-cdb9-4e6f-940c-e1b66350da90",
   "metadata": {},
   "source": [
    "# Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bbcd7f-e1af-4c05-b024-e69d82a899de",
   "metadata": {},
   "source": [
    "Transformers come in three main flavors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb6e538-a31b-470b-a973-6e8863794273",
   "metadata": {},
   "source": [
    "## Flavors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de47a51-d28c-4d0e-bad7-60b7afc3ca4f",
   "metadata": {},
   "source": [
    "### Encoder-decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a3e5e-8580-45e2-8543-4858f865716a",
   "metadata": {},
   "source": [
    "![image.png](../_static/post/another-annotated-transformer/tformer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf612922-1b05-4c6a-b25e-6528fdc4bcbb",
   "metadata": {},
   "source": [
    "* These models are officially just this diagram.\n",
    "* They're of a class of models called [seq2seq](https://en.wikipedia.org/wiki/Seq2seq) models.\n",
    "* They take sequence inputs, generate some state features (via the encoder), and generate a sequence output (via the decoder).\n",
    "* As such, they're typically used as translation models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6ab38194-1290-43fd-91c2-9b04266a91e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderDecoderTransformer(nn.Module):\n",
    "    pos_encoding: Callable[[int, int], jnp.array]\n",
    "    in_vocab_size: int\n",
    "    out_vocab_size: int\n",
    "    embed_dim: int\n",
    "    n_layers: int\n",
    "    hidden_dim: int\n",
    "    attn_dropout: float\n",
    "    fc_dropout: float\n",
    "    n_heads: int\n",
    "    size_per_head: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.encoder = Encoder(\n",
    "            pos_encoding=self.pos_encoding,\n",
    "            vocab_size=self.in_vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            layers=[EncoderLayer(hidden_dim=self.hidden_dim,\n",
    "                                 attn_dropout=self.attn_dropout,\n",
    "                                 fc_dropout=self.fc_dropout,\n",
    "                                 n_heads=self.n_heads,\n",
    "                                 size_per_head=self.size_per_head,\n",
    "                                 name=f'encoder_{i}')\n",
    "                    for i in range(self.n_layers)])\n",
    "        self.decoder = Decoder(\n",
    "            pos_encoding=self.pos_encoding,\n",
    "            vocab_size=self.out_vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            layers=[DecoderLayer(hidden_dim=self.hidden_dim,\n",
    "                                 attn_dropout=self.attn_dropout,\n",
    "                                 fc_dropout=self.fc_dropout,\n",
    "                                 n_heads=self.n_heads,\n",
    "                                 size_per_head=self.size_per_head,\n",
    "                                 name=f'decoder_{i}')\n",
    "                    for i in range(self.n_layers)])\n",
    "\n",
    "    def __call__(self, X, Y, source_mask, training=False):\n",
    "        # required for dot product attention\n",
    "        chex.assert_equal(self.encoder.embed_dim, self.decoder.embed_dim)\n",
    "        encodings = self.encoder(X, source_mask, training=training)\n",
    "        self.sow('intermediates', 'encodings', encodings)\n",
    "        return self.decoder(encodings, Y, source_mask, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284a397-bdf8-4276-aa20-12316552b889",
   "metadata": {},
   "source": [
    "### Encoder-only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf04ab8-62b1-4fd5-9608-dd8d838ea78b",
   "metadata": {},
   "source": [
    "![image.png](../_static/post/another-annotated-transformer/encoder-circled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61380cc-1e1b-4649-a473-df3ac2835395",
   "metadata": {},
   "source": [
    "* These take in a sequence and output state features.\n",
    "* It's mostly useful for tasks like text classification, sentiment analysis, stuff like that.\n",
    "* One notable example is Google's [bert](https://en.wikipedia.org/wiki/BERT_(language_model))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d08420e-2031-4233-85b5-91f9ae76e14f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderOnlyTransformer(nn.Module):\n",
    "    pos_encoding: Callable[[int, int], jnp.array]\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "    n_layers: int\n",
    "    hidden_dim: int\n",
    "    attn_dropout: float\n",
    "    fc_dropout: float\n",
    "    n_heads: int\n",
    "    size_per_head: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.encoder = Encoder(\n",
    "            pos_encoding=self.pos_encoding,\n",
    "            vocab_size=self.vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            layers=[EncoderLayer(hidden_dim=self.hidden_dim,\n",
    "                                 attn_dropout=self.attn_dropout,\n",
    "                                 fc_dropout=self.fc_dropout,\n",
    "                                 n_heads=self.n_heads,\n",
    "                                 size_per_head=self.size_per_head,\n",
    "                                 name=f'encoder_{i}')\n",
    "                    for i in range(self.n_layers)])\n",
    "\n",
    "    def __call__(self, X, mask, training=False):\n",
    "        return self.encoder(X, mask, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abd088f-9882-4274-82f5-75ff272b54b8",
   "metadata": {},
   "source": [
    "### Decoder-only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6dc69-0174-4c2d-8454-3a0629988c8c",
   "metadata": {},
   "source": [
    "![image.png](../_static/post/another-annotated-transformer/decoder-circled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217e9fd-11aa-46bc-964f-c7fc665cb11d",
   "metadata": {},
   "source": [
    "* These are called [generative model](https://en.wikipedia.org/wiki/Generative_model)s.\n",
    "* They take a static state and generate a sequence iteratively.\n",
    "* Mostly useful for text (media) generation[^transformer_uses].\n",
    "* One notable example: [GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer).\n",
    "\n",
    "[^transformer_uses]: This fact is very quickly becoming outdated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "44affc29-f45d-4e0c-b6e0-ed73236c4294",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderOnlyTransformer(nn.Module):\n",
    "    pos_encoding: Callable[[int, int], jnp.array]\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "    n_layers: int\n",
    "    hidden_dim: int\n",
    "    attn_dropout: float\n",
    "    fc_dropout: float\n",
    "    n_heads: int\n",
    "    size_per_head: int\n",
    "\n",
    "    def setup(self):\n",
    "        self.embed = nn.Embed(self.vocab_size, self.embed_dim)\n",
    "        self.decoder = Decoder(\n",
    "            pos_encoding=self.pos_encoding,\n",
    "            vocab_size=self.out_vocab_size,\n",
    "            embed_dim=self.embed_dim,\n",
    "            layers=[DecoderLayer(hidden_dim=self.hidden_dim,\n",
    "                                 attn_dropout=self.attn_dropout,\n",
    "                                 fc_dropout=self.fc_dropout,\n",
    "                                 n_heads=self.n_heads,\n",
    "                                 size_per_head=self.size_per_head,\n",
    "                                 name=f'decoder_{i}')\n",
    "                    for i in range(self.n_layers)])\n",
    "\n",
    "    def __call__(self, static, X, source_mask, training=False):\n",
    "        encodings = self.embed(static)\n",
    "        return self.decoder(encodings, X, source_mask, training=training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e1c650-d448-43cd-9b2b-4f6ec5a0149d",
   "metadata": {},
   "source": [
    "# Example\n",
    "As an example, we'll train a model to perform rot13. This isn't intended to be an example how how these models can be useful, but rather just an example of how this model in training. There are a few reasons why this task is not appropriate, but the biggest one is probably that from the perspective of the model, this is not much different from a copy task[^identity_fn].\n",
    "\n",
    "[^identity_fn]: The copy task is the identity function. You train a model to copy the input ($f(X) = X$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc7093d-fd29-46d3-949f-730b22a96845",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Jax requires a bit of setup, so we'll do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3dd95f4-20c4-4f7b-9883-e31092bb094d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/amniskin/.local/share/hatch/env/virtual/annotated-transformer-Cfjl09Ky/annotated-transformer/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d36f2aed-1cd1-4a07-8d9e-8bcec456ec1c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0,\n",
       " 'b': 1,\n",
       " 'c': 2,\n",
       " 'd': 3,\n",
       " 'e': 4,\n",
       " 'f': 5,\n",
       " 'g': 6,\n",
       " 'h': 7,\n",
       " 'i': 8,\n",
       " 'j': 9,\n",
       " 'k': 10,\n",
       " 'l': 11,\n",
       " 'm': 12,\n",
       " 'n': 13,\n",
       " 'o': 14,\n",
       " 'p': 15,\n",
       " 'q': 16,\n",
       " 'r': 17,\n",
       " 's': 18,\n",
       " 't': 19,\n",
       " 'u': 20,\n",
       " 'v': 21,\n",
       " 'w': 22,\n",
       " 'x': 23,\n",
       " 'y': 24,\n",
       " 'z': 25,\n",
       " '<start>': 26,\n",
       " '<pad>': 27}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {chr(97 + i): i for i in range(26)}\n",
    "vocab['<start>'] = len(vocab)\n",
    "vocab['<pad>'] = len(vocab)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b643c96-604a-4042-967a-0264c61cd4a5",
   "metadata": {},
   "source": [
    "Throughout this example, we'll be using the fact that the token IDs correspond to the sorted alphabet with two tokens added at the end[^alphabet_index]. So we do things like generate random strings and compute the target via modulo arithmetic.\n",
    "\n",
    "[^alphabet_index]: Meaning `vocab['a'] == 0`, and `vocab['b'] == 1`, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "efd0f98b-e785-4df9-8516-8e0a3bdee609",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(key):\n",
    "    k0, k1 = jran.split(key, 2)\n",
    "    max_len = 15\n",
    "    X = jran.randint(k0, (10, max_len), 0, len(vocab) - 2)\n",
    "    mask = jnp.stack([jnp.arange(max_len) >= i for i in jran.randint(k1, (10,), 1, max_len)])\n",
    "    X = X * (1 - mask) + (mask * vocab['<pad>'])\n",
    "    Y = ((X + 13) % (len(vocab) - 2)) * (1 - mask) + mask * vocab['<pad>']\n",
    "    Ys = (\n",
    "        jnp.ones_like(Y, dtype=jnp.int32)\n",
    "        .at[:, 1:].set(Y[:, :-1])\n",
    "        .at[:, 0].set(vocab['<start>'])\n",
    "    )\n",
    "    return (X, Ys, mask.astype(jnp.float32)), Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e11a0103-98b3-417f-be28-9f891a863c70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_params:  4665\n"
     ]
    }
   ],
   "source": [
    "mdl = EncoderDecoderTransformer(pos_encoding=sin_pos_enc,\n",
    "                                in_vocab_size=len(vocab),\n",
    "                                out_vocab_size=len(vocab),\n",
    "                                embed_dim=8,\n",
    "                                n_layers=1,\n",
    "                                hidden_dim=5,\n",
    "                                attn_dropout=0.0,\n",
    "                                fc_dropout=0.0,\n",
    "                                n_heads=7,\n",
    "                                size_per_head=5)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1),\n",
    "    optax.sgd(\n",
    "        learning_rate=optax.warmup_exponential_decay_schedule(\n",
    "            init_value=0.5, peak_value=0.8, warmup_steps=100,\n",
    "            transition_steps=200, decay_rate=0.5,\n",
    "            transition_begin=100, staircase=False, end_value=1e-3\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "params = mdl.init(key, *get_data(key)[0])\n",
    "print('num_params: ', num_params(params))\n",
    "opt_state = opt.init(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5538b60-dcca-4aee-af75-31668ee2f0c5",
   "metadata": {},
   "source": [
    "Notice that we have almost 5 thousand parameters and this is just about the minimal example I could come up with. This architecture gets big real quick. Our `train_step` function is our main training code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bb879382-9e36-405f-be3c-6a552ccba54c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def train_step(params, opt_state, step, key):\n",
    "    \"\"\"Train for a single step.\"\"\"\n",
    "    k0, k1 = jran.split(jran.fold_in(key, step))\n",
    "    args, y = get_data(k0)\n",
    "\n",
    "    @jax.grad\n",
    "    def grad_fn(params):\n",
    "        logits = mdl.apply(params, *args,\n",
    "                           training=True, rngs={'dropout': k1})\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "            logits, y\n",
    "        ).mean()\n",
    "        return loss\n",
    "    grads = grad_fn(params)\n",
    "    updates, opt_state = opt.update(\n",
    "        grads, opt_state, params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "529edaf5-3dd5-45b6-b86f-4a35b106bc14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:26<00:00, 371.81it/s]\n"
     ]
    }
   ],
   "source": [
    "for step in tqdm(range(10_000)):\n",
    "    params, opt_params = train_step(params, opt_state, step, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35ca6e1f-891a-44be-a751-d6ecfd2cb9b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args, y = get_data(key + 500)\n",
    "yh = mdl.apply(params, *args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "34f90bd3-2192-4cc1-94e4-35ebf61c0308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(True, dtype=bool)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y[:, :-1] == jnp.argmax(yh[:, :-1], axis=-1)).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df90ae46-5e65-49a3-91cf-38c79cf84a44",
   "metadata": {},
   "source": [
    "Now let's check it out on our own words. To do that we'll write our own `rot13` and `rot13_inv` functions[^bad_idea].\n",
    "\n",
    "[^bad_idea]: Yet another reason why this is a bad idea (we have easier to reason about alternatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2031e27-8107-4e6e-b4ac-2ace872bfd1f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdfqwerz => nfqsdjrem => asdfqwerz\n"
     ]
    }
   ],
   "source": [
    "def rot13(input_string):\n",
    "    return ''.join([chr(((vocab[x] + 13) % 26) + 97) for x in input_string])\n",
    "def rot13_inv(input_string):\n",
    "    return ''.join([chr(((vocab[x] - 13) % 26) + 97) for x in input_string])\n",
    "\n",
    "a = 'asdfqwerz'\n",
    "b = rot13(a)\n",
    "c = rot13_inv(b)\n",
    "print(a, '=>', b, '=>', c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4b8845ce-d615-4a41-8508-43e18423de1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0, 18,  3,  5, 27, 27],\n",
       "       [16, 22,  4, 17, 27, 27],\n",
       "       [25, 23,  2, 21,  1, 20]], dtype=int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def str2ids(txt):\n",
    "    return [vocab[x] for x in txt]\n",
    "\n",
    "\n",
    "def strs2ids(*txts):\n",
    "    ids = [str2ids(x) for x in txts]\n",
    "    maxlen = max([len(x) for x in ids])\n",
    "    return jnp.stack([jnp.pad(jnp.array(x), (0, maxlen - len(x)), 'constant', constant_values=vocab['<pad>'])\n",
    "                      for x in ids])\n",
    "\n",
    "strs2ids('asdf', 'qwer', 'zxcvbu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e41d7604-3564-48b1-b8ff-0b0cb4033adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ids2str(ids):\n",
    "    x = [list(vocab)[x] for x in ids]\n",
    "    x = [y if y != '<pad>' else '~' for y in x]\n",
    "    return ''.join(x).rstrip('~')\n",
    "\n",
    "def ids2strs(ids):\n",
    "    return [ids2str(x) for x in ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48a55ac-a594-479f-a23c-62deec31d530",
   "metadata": {},
   "source": [
    "Now let's run the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9211b173-e7c7-45f2-ac61-ebb2ba446b92",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = jnp.array(strs2ids('hey', 'there', 'ma', 'dood'))\n",
    "start = jnp.array([[vocab['<start>']]] * X.shape[0], dtype=jnp.int32)\n",
    "Y = start\n",
    "while (Y[:, -1] != vocab['<pad>']).any():\n",
    "    Y = jnp.argmax(mdl.apply(params, X, jnp.concatenate([start, Y], axis=-1), X == vocab['<pad>']), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98310aba-9f14-4fe6-8cbc-6452dd14bead",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['url', 'gurer', 'zn', 'qbbq']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids2strs(list(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "34d7cdeb-7c5f-416b-a278-326da2673f27",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hey', 'there', 'ma', 'dood']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[rot13_inv(x) for x in ids2strs(list(Y))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393f9ca0-e524-4859-8323-7477be4b2b28",
   "metadata": {},
   "source": [
    "Yay! It works! We can see that the encoder transforms the input sequence into input embeddings, whereas the decoder transforms a state vector (attending to its own history) to the target.\n",
    "\n",
    "Stay tuned for more on transformers. Future topics include:\n",
    "\n",
    "1. Ablation study to see what each part contributes\n",
    "2. Model explainability\n",
    "3. How to implement many tasks as either encoders, decoders, encoder-decoders"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "annotated-transformer",
   "language": "python",
   "name": "annotated-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
